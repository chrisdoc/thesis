
\chapter{Implementation}
\label{cha:implementation}
The following chapter will present the actually implemented system. The implementation is based on Chapter~\ref{cha:concepts} where the concepts for the system have been presented. Through out this chapter all parts of the system and their responsibilities will be explained.  
\section{System Overview}
The developer system consists of a set of individual systems. This systems can be grouped in three groups. The first is responsible for storing information such as: mobile subscription data, population and land use information, road network and cell coverage areas. The second is a data abstraction layer which is used to aggregate the data and present to interested systems via an API. The last group is responsible for processing and generating trajectories. It is using the other systems via the API provided by the data abstraction layer. A brief overview of the implemented system is illustrated in Figure~\ref{fig:systemoverview}.
\begin{figure}
\centering
\includegraphics[width=\linewidth]{./images/systemoverview}
\caption{System overview of the implemented system}
\label{fig:systemoverview}
\end{figure}
It depicts the data store which consists of two different databases, the first is responsible for storing geographic information and the second for storing subscription information from A1 and OCM.
\subsection{Data Store}
The data store is responsible for storing data, this data either is either geographic information or subscription data. For geographic data the system is using PostgreSQL with the PostGIS extension. PostGIS extends PostgreSQL with support for spatial data structures and topological queries~\cite{Obe2011}. Therefore, it allows the system to store the OpenStreetMap road network, population and land use information, the coverage area and the location of each cell site. PostGIS allows the use of advanced spatial queries as intersections and clipping which is later needed by the system.

Subscription data is stored in a separate MySQL database. It is storing events captured by the A1 system and the OpenCoverageMap project. The database consists of two tables, one for A1 and the second one for OpenCoverageMap.
\subsection{Data Abstraction API}
We first started to develop a system with direct access to the database via the JDBC driver for PostgresSQL\footnote{PostgreSQL JDBC driver: \url{http://jdbc.postgresql.org/}, last accessed March, 28 2014} and MySQL\footnote{MySQL JDBC driver: \url{https://dev.mysql.com/downloads/connector/j/}, last accessed March, 28 2014}. But this approach was not sufficient as we experienced a slow data access rate with larger queries. Therefore, we decided to build an abstraction layer which encapsulates the access of the database form the system. The abstraction layer is built with Node.js -- a JavaScript framework -- and it provides a HTTP API. More details will be provided in Section~\ref{sec:architecture}.
\subsection{Trajectory Generation}
The trajectory generation group consists of two systems. First the trajectory service which is generating trajectories for subscribers and second the routing engine which is responsible for calculating routes between a start and end point. 
\section{Architecture}
\label{sec:architecture}
After a brief overview has been given in the previous section, here the underlying system will be explained in more detail. It will also cover the process of importing the needed data into the database and how this data can be retrieved from the database. Moreover the used data and the communication between the subsystems will be described.
\subsection{Databases}
\subsubsection{PostgreSQL Database}
To store geographic information PostgreSQL together with the PostGIS extension is used. PostGIS extends PostgreSQL in a way that it follows the Simple Features for SQL specification from the Open Geospatial Consortium. The Simple Features are defined in ISO 19125~\cite{ISO19125,ISO191252}. The developed system is using population and land use information stored in the database. This data is needed to estimate a subscribers start and end position within the boundaries of the cell coverage area. In addition to raster data (population and land use) the database also stores the OpenStreetMap road network. Figure~\ref{fig:pg_structure} depicts the internal structure of the PostgreSQL database. The table \emph{at_2po_4pgr} contains the OpenStreetMap road network, \emph{landuse_raster} land use information and \emph{density_raster} population information. 
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{./images/pg_structure}
\caption{Structure of the PostgreSQL database}
\label{fig:pg_structure}
\end{figure}

\paragraph{Import Data}
This section describes the process to import both raster and the a road network into the database. Raster data is simply an image where each pixel corresponds to a coordinate in world space. A raster image defines a grid spacing which defines how far in meters pixel are away. A grid spacing of $20$ meters means that each pixel covers an area of $400$ square meters. Usually environmental and demographic geographic information, is stored as a raster image. On the other hand road networks consists of a vector of points. The road network is stored in the database as Simple Features.

\paragraph{Raster}
To import raster data into the PostgreSQL database the tool \verb|raster2pgsql| was used. Listing~\ref{lst:raster2pgsql} illustrates how to import the data. The parameter \verb|s| defines the spatial reference system of the raster image. The grid spacing of the raster image is defined by parameter \verb|t| and is in this case $100$ by $100$ meters. The tool converts the raster image into an SQL file which can be inserted into the database. To insert the created into the database the second command is used.
\begin{lstlisting}[style=BashInputStyle,caption={Import raster data into PostgreSQL database},label={lst:raster2pgsql}]
    # raster2pgsql -s 4236 -I -C -M *.tif -F -t 100x100 population > pop.sql
    # psql -U postgres -d gisdb -f pop.sql
\end{lstlisting}
\paragraph{Road Network}
Our system is using the routing engine osm2po which operates on the OpenStreetMap road network. Osm2po can convert the OpenStreetMap road network to a PostgreSQL compatible SQL file. Listing~\ref{lst:osm2pgsql} shows the step to import an OpenStreetMap road network -- in this case from Austria -- into the PostgreSQL  database.
\begin{lstlisting}[style=BashInputStyle,caption={Import OpenStreetmap data into PostgreSQL database},label={lst:osm2pgsql}]
    # java -jar osm2po-4_8_8.jar prefix=at austria.osm.pbf
    # psql -U postgres -d gisdb -f at/at_2po_4pgr.sql.sql
\end{lstlisting}
\subsubsection{MySQL Database}
The developed system is using data from A1 and the OpenCoverageMap project. Because this data is not related with geographic information a second database is used to store this data. This information is stored in two separate tables because the structure of A1 and the OpenCoverageMap is different. Figure~\ref{fig:mysql_structure} depicts the structure of the two tables, the first \emph{mfd_events} stores the A1 data and the second \emph{ocm_all} stores information from the OpenCoverageMap project.
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{./images/mysql_structure}
\caption{Structure of the MySQL database}
\label{fig:mysql_structure}
\end{figure}

\paragraph{Import A1}
To import the A1 data set into the MySQL, the binary event stream needs to be converted first. The conversion takes the binary event stream and converts its to CSV file. Each row in the CSV file represents an event. Later the created CSV file will be imported into the database similar to Listing~\ref{lst:textmysql}.
\paragraph{Convert}
The conversion is carried out a by a Java program which takes the binary data stream as input and outputs a CSV file. First a Java \verb|InputStream| from the input file will be created. This input stream will be passed to a \verb|DataInputStream|. As illustrated in Listing~\ref{lst:parsestream} a loop iterates over the data input stream and parses the binary information according to the A1 data structure. At last at the end of each iteration the parsed information will be print to the standard output.


\begin{lstlisting}[language=Java,style=JAVA,caption={Example code to parse event data stream},label={lst:parsestream}]
// create file input stream
InputStream is = new FileInputStream("a1.data");
// create new data input stream
DataInputStream dis = new DataInputStream(is);
while (dis.available() != 0){
    byte[] anon_id = new byte[32];
    dis.read(anon_id);
    timestamp=dis.readInt();
    timestamp_ms=dis.readInt();
    lac=dis.readInt();
    cell_id=dist.readInt();
    latitude=dis.readDouble();
    longitude=dis.readDouble();
    radius= dis.readLong();
    dis.skipBytes(12); //skip reserved fields
    event_type = dis.readShort();
    dis.skipBytes(3); //skip reserved fields
    angle_x=dis.readInt();
    angle_y=dis.readInt();
    speed=dis.readInt();
    //only print useful fields
    System.out.println(Hex.encodeHexString(anon_id) +";"+ timestamp +";"+lac +";"+cell_id +";"+latitude +";"+longitude +";"+event_type +";"+angle_x);
}
\end{lstlisting}
\begin{lstlisting}[style=htmlcssjs,language=SQL,caption={Load text file into MySQL database},label={lst:textmysql}]
LOAD DATA INFILE 'data.txt' INTO TABLE tbl_name
  FIELDS TERMINATED BY ',' ENCLOSED BY '"'
  LINES TERMINATED BY '\n';
\end{lstlisting}

\paragraph{Import OpenCoverageMap}
The data we got provided by the OpenCoverageMap project was already a CSV file and can be imported into the database with an SQL statement similar to the one in Listing~\ref{lst:textmysql}.
%\begin{table}
%    \begin{tabular}{llllll}
%    timestamp\_android & tracks\_id & tel\_cellid & tel\_lac & gps\_latitude    & gps\_longitude   \\
%    1327678831        & 4         & 51520      & 5501    & 48.366327106511  & 14.5197825943963 \\
%    1327678834        & 4         & 51510      & 5501    & 48.3660673271494 & 14.5201790807638 \\
%    1327678835        & 4         & 51510      & 5501    & 48.3659146718706 & 14.5203579418855 \\
%    1327678836        & 4         & 51510      & 5501    & 48.365833326414  & 14.5204686022062 \\
%    \end{tabular}
%\end{table}
\subsection{Node.js Data API}
To encapsulate the querying of the database from the trajectory service a Node.js API was implemented. It provides a HTTP API which gives access to the data stored in the PostgreSQL and MySQL database. The data API is using the following modules, which can be installed through the script provided in Listing~\ref{lst:nodemodules}:
\begin{itemize}
\item \textbf{express}\footnote{Project website with more information about express: \url{http://expressjs.com}, last accessed March 28, 2014} is a web application for Node.js and provides our system with web request routing in a RESTful manner
\item \textbf{pg}\footnote{More information about the PostgreSQL driver pg: \url{https://github.com/brianc/node-postgres}, last accessed March 28, 2014} is a PostgreSQL driver for Node.js
\item \textbf{mysql}\footnote{Project website with more information and a tutorial about mysql: \url{https://github.com/felixge/node-mysql}, last accessed March 28, 2014} is a MySQL driver for Node.js
\end{itemize}
\begin{lstlisting}[style=BashInputStyle,caption={Script to install the used Node.js modules},label={lst:nodemodules}]
    # npm install express
    # npm install pg
    # npm install mysql
\end{lstlisting}

\begin{lstlisting}[style=htmlcssjs,language=JavaScript,caption={Example for a Node.js app which is using the express framework and the PostgreSQL and MySQL module},label={lst:nodeexpress}]
var pg = require('pg');

var conString = "postgres://postgres:1234@localhost/postgres";
var mysql = require('mysql');
var connection = mysql.createConnection({
    host: 'localhost',
    user: 'mysql',
    password: '1234'
});

var express = require('express'),
    app = express.createServer();

app.get('/mysql', function (req, res) {
    connection.connect();
    connection.query('SELECT 1 + 1 AS result', function (err, rows, fields) {
        res.send(rows[0].solution);
    });
    connection.end();
});

app.get('/postgres', function (req, res) {
    var client = new pg.Client(conString);
    client.connect(function (err) {
        client.query('SELECT NOW() AS time', function (err, result) {
            res.send(result.rows[0].time);
        });
    });
});

app.listen(3000);
\end{lstlisting} 

\subsubsection{Features}
The Node.js provides several features  to the trajectory service. It enables the trajectory service to access the two database via an uniform API. This API hides the implementation from the using program. However, to increase performance the API was split into two parts. The first API resides on the same server as the PostgreSQL and handles geographic request whereas the second one handles subscription data and resides on the MySQL server.

\paragraph{MySQL API}
provides access to mobile subscription data for both the A1 and OpenCoverageMap data. In addition, the route traveled by a subscriber can be retrieved from the OpenCoverageMap data. This route consists of a timestamp and the latitude and longitude coordinate. All the returned data is encoded as JSON that eases the parsing. Listing~\ref{lst:mysqlapireq} depicts the request that can be made to the database. The first one retrieves events from A1 for the specified user, the second one retrieves events from OpenCoverageMap and the last one get the route traveled by the OpenCoverageMap user.
\begin{lstlisting}[style=BashInputStyle,caption={MySQL API requests},label={lst:mysqlapireq}]
GET /mfdevents?id=USER_ID
GET /ocmraw?id=USER_ID
GET /ocmroute?id=USER_ID
\end{lstlisting}

\paragraph{PostgreSQL API} 
enables the requesting program to access functions of the PostgreSQL database. It provides access to raster data as population and land use information as well as to retrieve a random point withing a specified area based on population and land use information (see Listing~\ref{lst:pgapireq}). The bounds parameter is encoded as a list  of coordinate -- separated by a comma -- specifying a bounding box (xmin,ymin,xmax,ymax). In comparison the geom parameter is encoded as Well-known text (WKT) which allows defining more advanced areas as polygons and multi polygons (POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))
). 
\begin{lstlisting}[style=BashInputStyle,caption={PostgreSQL API requests},label={lst:pgapireq}]
GET /populationBounds?bounds=BOUNDARIES
GET /landuseBounds?bounds=BOUNDARIES
GET /randomgeom?geom=GEOMETRY&num=NUMBER_OF_POINTS
\end{lstlisting}


\subsection{Routing Engine}
To calculate routes on the OpenStreetMap road network, the system is using the osm2po routing engine. This engine can either be used as a standalone web server with a defined API or be integrated into a Java application. In our case the engine was integrated into the trajectory service as a library. The benefit of this approach is, that the system has better access to the graph file used by the engine. The graph file is a reduced road network optimized for routing. By accessing the graph file, the trajectory service can retrieve metadata for the calculated route such as speed limits and length of the roads. This information can later be used to validate the calculate route and the timing information. 

Listing~\ref{lst:osm2po} illustrates how a route between Linz and Hagenberg im Mühlkreis is calculated. The result is an array with segment IDs that can then be used to retrieve the geometry and the speed limits of the segment from the graph file.
\begin{lstlisting}[language=Java,style=JAVA,caption={Example code to calculate a route between two points},label={lst:osm2po}]
File graphFile = new File("at_2po_4pgr.gph");
Graph graph = new Graph(graphFile);
DefaultRouter router = new DefaultRouter();

//Linz, Austria
int sourceId = graph.findClosestVertexId(48.306f,14.285f);
//Hagenberg im Mühlkreis, Austria
int targetId = graph.findClosestVertexId(48.368f,14.513f);

Properties params = new Properties();
params.setProperty("findShortestPath", "false");

//calculate path between source and target
int[] path = router.findPath(
    graph, sourceId, targetId, Float.MAX_VALUE, params);
\end{lstlisting}
\subsection{Trajectory Service}
The trajectory service is using the above mentioned data and APIs to generate trajectories for each subscriber. By using different data sources it can estimate trajectories for different data providers in our case mobile network operators. In the current state trajectories can be generated for A1 subscriber and OpenCoverageMap participants. To fulfill its task the trajectory service needs besides the events from the network operator a coverage prediction for each cell site. The trajectory service is using the following data:
\begin{itemize}
\item Mobile subscription data
\item Population and land use information
\item Cell site coverage prediction
\item OpenStreetMap road network
\item osm2po route engine
\end{itemize}

\subsubsection{Convert Events}
The structure of events used in the trajectory generator was inspired by the structure used in the A1 event stream. To integrate other data sources such as the OpenCoverageMap a converted needs to be implemented. The objective of the converter is to transform the event structure to the one used by A1 events.

The OpenCoverageMap doesn't use the same event structure as A1. Rather than issuing events like call establish / termination or hand over it periodically stores the state of the mobile station. Therefore, this information needs to be transformed to an event structure. The steps to convert the OpenCoverageMap data to the A1 structure are the following: First remove all data where no call is active. Second iterate over this data and check for gaps between succeeding timestamps. Gaps greater than 10 seconds indicate that a call was either established or terminated. Afterwards iterate over the data between two gaps. Create a call establish and termination event at the beginning and end of the list. If there is a difference between the cell-id and location area of two succeeding entries add a hand over event to the current call. The result will be a  list of events with the same structure as the ones captured by A1.
%\begin{enumerate}
%\item remove data where no call is active
%\item iterate over all events
%\item compare cellid and lac of event $i$ and $i+1$
%\item compare 
%\item compare timestamp of event $i$ and $i+1$
%\item if timestamp difference $>10$ seconds create new call otherwise add event to previous call
%\end{enumerate}

%\begin{algorithm}[H]
% \KwData{this text}
% \KwResult{how to write algorithm with \LaTeX2e }
% initialization\;
% \While{not at end of this document}{
%  read current\;
%  \eIf{understand}{
%   go to next section\;
%   current section becomes this one\;
%   }{
%   go back to the beginning of current section\;
%  }
% }
% \caption{How to write algorithms}
%\end{algorithm}

%\begin{algorithm}
%\caption{Conversion algorithm}\label{euclid}
%\begin{algorithmic}[1]
%\Procedure{ConvertOCM}{$ocm$}\Comment{Convert OCM to A1}
%\State $n:=length(ocm);$
%\State $calls \gets list$
%\For{i:=0}{n-1}
%	\State $timestamp_{now}:=ocm(i).timestamp;$
%	\State $timestamp_{next}:=ocm(i+1).timestamp;$
%		\State $laccellid{now}:=ocm(i).laccellid;$
%		\State $laccellid{next}:=ocm(i+1).laccellid;$
%	\If{$timestamp_{next}-timestamp_{now}>10$}
%	\State $add(calls,call)$
%	\EndIf
%\EndFor
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}

\section{Trajectory Service}
The first two system layers were responsible for storing and providing access to various data sources, this layer however, is utilizing the data. The trajectory service implements the trajectory generation pipeline that has been introduced in Chapter ~\ref{cha:concepts}. The application provides a rich user interface which offers functionalities that allows investigating the used coverage prediction and generating subscriber trajectories. In this section, the most important aspects of the generation pipeline are explained.
\subsection{Start and End Position}
\subsection{Route Validation}
\subsubsection{Geometry}
\subsubsection{Timing}
\subsection{Timing Information}
\subsubsection{Handover Prediction}
%\section{Call detection / filtering}
%\subsection{Call Length Distribution}
%\section{Stat and End Position}
%\subsection{Proof of Hypothesis}
%\subsection{Population and Land Use Information}
%\subsubsection{Population Density}
%\subsubsection{CLC}
%\subsection{Polygon Triangulation}
%\subsection{Random Point Distribution}
%\subsection{PL/R Interface}
%\section{Routing}
%\subsection{Origin Destination Routing}
%\subsection{Route Validation}\label{sec:route-validation}
%\subsubsection{Time Ratio}
%\subsubsection{Euclidean Distance}
%\subsubsection{Hausdorff metric}
%\section{Event clustering}
%\subsubsection{Geometric Clustering}
%\subsubsection{Event Clustering}
%\section{Time Synchronization}
%\section{Coverage Analysis}
%\subsection{Radio Network Planning}
%\subsection{Digital Elevation Model}
%\subsection{Buildings}
%\subsection{Radio Signal Strength}
